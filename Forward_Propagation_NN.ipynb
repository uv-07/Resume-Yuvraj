{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Implement the forward propagation for a two hidden layer network for m-samples, n-features as we discussed in class. Initialize the weights randomly. Use the data from the previous labs like logistic regression. You can choose the number of neurons in the hidden layer and use sigmoid activation function.Report the evaluation metrics for the network.  Also use other non-linear activation functions like ReLU and Tanh. Report the loss using both MSE and Cross Entropy."
      ],
      "metadata": {
        "id": "738iBKbrPRgo"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mqnT4pWYPL_-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fb2b9a65-eb64-4d49-a916-be1eef24bfc9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1.32,2.67,0\n",
            "2.68,1.25,0\n",
            "2.32,4.81,0\n",
            "2.65,1.3,0\n",
            "4.73,2.97,0\n",
            "3.94,2.79,0\n",
            "4.52,1.98,0\n",
            "1.29,3.85,0\n",
            "3.92,3.05,0\n",
            "2.4,4.33,0\n",
            "7.35,7.58,1\n",
            "8.14,7.97,1\n",
            "6.04,6.86,1\n",
            "5.34,7.67,1\n",
            "8.3,7.87,1\n",
            "7.52,7.16,1\n",
            "8.9,6.78,1\n",
            "8.08,6.73,1\n",
            "8.23,6.28,1\n",
            "6.92,5.19,1\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import csv\n",
        "\n",
        "def numgenerator(c = list,r = int,n = int,k = int):\n",
        "  numbers = []\n",
        "  while len(numbers)< n:\n",
        "    a1= round(np.random.rand()*15,2)\n",
        "    a2= round(np.random.rand()*15,2)\n",
        "    if np.sqrt(np.square(a1-c[0]) + np.square(a2 - c[1])) < r:\n",
        "      if [a1,a2] not in numbers:\n",
        "        numbers.append([a1,a2,k])\n",
        "      else:\n",
        "         continue\n",
        "  return numbers\n",
        "\n",
        "data1 = numgenerator([3,3],2,10,0)\n",
        "data2 = numgenerator([7,7],2,10,1)\n",
        "# data3 = numgenerator([11,11],2,10)\n",
        "data = data1 + data2\n",
        "\n",
        "with open('datapoints.csv', 'w', encoding='UTF8') as f:\n",
        "    writer = csv.writer(f)\n",
        "    for i in data:\n",
        "      writer.writerow(i)\n",
        "\n",
        "\n",
        "f = open('datapoints.csv', 'r')\n",
        "a = f.read()\n",
        "print(a)\n",
        "f.close()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "\n",
        "# Read the data\n",
        "head = ['x1', 'x2','y']\n",
        "df = pd.read_csv('datapoints.csv', names=head, header=None)\n",
        "# print(df)\n",
        "\n",
        "ones = np.ones(len(df))\n",
        "A0 = np.column_stack((ones, df.x1, df.x2))\n",
        "# print(df)\n",
        "\n",
        "# Sigmoid function\n",
        "def sigmoid(z):\n",
        "    return 1 / (1 + np.exp(-z))\n",
        "\n",
        "#ReLU Function\n",
        "def relu(z):\n",
        "  return np.maximum(0,z)\n",
        "\n",
        "#Tanh function\n",
        "def tanh(z):\n",
        "  return (np.exp(z)-np.exp(-z))/(np.exp(z)+np.exp(-z))\n",
        "\n",
        "def weights(n,k):\n",
        "  np.random.seed(k)\n",
        "  return np.random.rand(n)\n",
        "\n",
        "# Hypothesis function\n",
        "def hypothesis(func,x, weights):\n",
        "    return func(np.dot(x, weights))\n",
        "\n",
        "def layer1(func, x):\n",
        "  a1 = []\n",
        "  for k in x:\n",
        "    a=[]\n",
        "    for i in range(4):\n",
        "      h = hypothesis(func,k, weights(3,i))\n",
        "      a.append(h)\n",
        "    a1.append(a)\n",
        "  ones = np.ones(len(a1))\n",
        "  a1 = pd.DataFrame(a1)\n",
        "  a1.insert(0,'ones', ones,)\n",
        "  return a1\n",
        "\n",
        "A1 = layer1(sigmoid, A0)\n",
        "print(A1)\n",
        "\n",
        "\n",
        "def layer2(func,x):\n",
        "  return (hypothesis(func,x,weights(5,0)))\n",
        "\n",
        "# Loss calculated using MSC function\n",
        "def loss_msc(a,b):\n",
        "  return np.sum(np.sqrt(np.square(a)-np.square(b)))\n",
        "\n",
        "#loss clculated using cross entropy function\n",
        "def loss_cross(a,b):\n",
        "  J = 0\n",
        "  for i in range(len(a)):\n",
        "    J = J + (-b[i]*np.log(sigmoid(a[i]))) - (1-b[i])*np.log(1-sigmoid(a[i]))/(len(a))\n",
        "\n",
        "  return J\n",
        "\n",
        "\n",
        "# Calculate evaluation metrics\n",
        "def evaluation(a,b):\n",
        "  tp = np.sum((a >= 0.5) & (b == 1))\n",
        "  tn = np.sum((a <= 0.5) & (b == 0))\n",
        "  fp = np.sum((a >= 0.5) & (b == 0))\n",
        "  fn = np.sum((a <= 0.5) & (b == 1))\n",
        "\n",
        "  precision = tp / (tp + fp)\n",
        "  recall = tp / (tp + fn)\n",
        "  f1_score = 2 * (precision * recall) / (precision + recall)\n",
        "  accuracy = (tp + tn) / len(b)\n",
        "\n",
        "  print(\"Precision:\", precision)\n",
        "  print(\"Recall:\", recall)\n",
        "  print(\"F1 Score:\", f1_score)\n",
        "  print(\"Accuracy:\", accuracy)\n",
        "\n",
        "\n",
        "\n",
        "#evaluation matrics for sigmoid function\n",
        "print('loss for Sigmoid as activation function')\n",
        "eval_sigmoid = layer2(sigmoid,A1)\n",
        "print(eval_sigmoid)\n",
        "loss_sig_msc = loss_msc(eval_sigmoid,df.y)\n",
        "loss_sig_cross = loss_cross(eval_sigmoid,df.y)\n",
        "print('Loss using sigmoid function and MSC:',loss_sig_msc)\n",
        "print('Loss using sigmoid function and Cross entropy function:',loss_sig_cross)\n",
        "evaluation(eval_sigmoid,df.y)\n",
        "\n",
        "#evaluation matrics for tanh function\n",
        "print()\n",
        "print('loss for tanh as activation function')\n",
        "eval_tanh = layer2(sigmoid,layer1(tanh,A0))\n",
        "loss_tanh_msc = loss_msc(df.y,eval_tanh)\n",
        "loss_tanh_cross = loss_cross(eval_tanh,df.y)\n",
        "print('Loss using sigmoid function and MSC:',loss_tanh_msc)\n",
        "print('Loss using sigmoid function and Cross entropy function:',loss_tanh_cross)\n",
        "evaluation(eval_tanh,df.y)\n",
        "\n",
        "#evaluation matrics for ReLU function\n",
        "print()\n",
        "print('loss for ReLU as activation function')\n",
        "eval_relu = layer2(sigmoid,layer1(relu,A0))\n",
        "loss_relu_msc = loss_msc(df.y,eval_relu)\n",
        "loss_relu_cross = loss_cross(eval_relu,df.y)\n",
        "print('Loss using sigmoid function and MSC:',loss_relu_msc)\n",
        "print('Loss using sigmoid function and Cross entropy function:',loss_relu_cross)\n",
        "evaluation(eval_relu,df.y)"
      ],
      "metadata": {
        "id": "suHlyheBSr5w",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3c46f9cb-8d2f-42d8-b3ea-27eda3b3f935"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    ones         0         1         2         3\n",
            "0    1.0  0.956985  0.797082  0.874112  0.905704\n",
            "1    1.0  0.961542  0.912746  0.767198  0.943335\n",
            "2    1.0  0.993984  0.889802  0.958514  0.973220\n",
            "3    1.0  0.961862  0.911010  0.771933  0.942976\n",
            "4    1.0  0.996737  0.978637  0.899450  0.991543\n",
            "5    1.0  0.993621  0.962867  0.888122  0.984519\n",
            "6    1.0  0.993138  0.975232  0.837741  0.986973\n",
            "7    1.0  0.977948  0.793586  0.929932  0.929844\n",
            "8    1.0  0.994463  0.962350  0.901508  0.985428\n",
            "9    1.0  0.992425  0.895322  0.946761  0.970970\n",
            "10   1.0  0.999969  0.996706  0.991781  0.999651\n",
            "11   1.0  0.999986  0.998133  0.993490  0.999822\n",
            "12   1.0  0.999877  0.991579  0.987424  0.998913\n",
            "13   1.0  0.999876  0.986135  0.991759  0.998591\n",
            "14   1.0  0.999987  0.998336  0.993153  0.999836\n",
            "15   1.0  0.999964  0.997084  0.989713  0.999651\n",
            "16   1.0  0.999983  0.998919  0.987793  0.999853\n",
            "17   1.0  0.999969  0.998050  0.987191  0.999734\n",
            "18   1.0  0.999964  0.998249  0.983718  0.999727\n",
            "19   1.0  0.999821  0.995514  0.969768  0.999053\n",
            "loss for Sigmoid as activation function\n",
            "[0.9291487  0.93114099 0.93880993 0.93124418 0.94057201 0.93939399\n",
            " 0.93828555 0.93259257 0.93984613 0.93851368 0.94420557 0.94430432\n",
            " 0.94389698 0.94384099 0.94430144 0.94415804 0.94416642 0.9441183\n",
            " 0.94402435 0.94351289]\n",
            "Loss using sigmoid function and MSC: 9.359547744824939\n",
            "Loss using sigmoid function and Cross entropy function: 3.9196100889308347\n",
            "Precision: 0.5\n",
            "Recall: 1.0\n",
            "F1 Score: 0.6666666666666666\n",
            "Accuracy: 0.5\n",
            "\n",
            "loss for tanh as activation function\n",
            "Loss using sigmoid function and MSC: 3.2839581312242156\n",
            "Loss using sigmoid function and Cross entropy function: 3.9202844890595316\n",
            "Precision: 0.5\n",
            "Recall: 1.0\n",
            "F1 Score: 0.6666666666666666\n",
            "Accuracy: 0.5\n",
            "\n",
            "loss for ReLU as activation function\n",
            "Loss using sigmoid function and MSC: 0.003698879025411993\n",
            "Loss using sigmoid function and Cross entropy function: 3.7888731490435648\n",
            "Precision: 0.5\n",
            "Recall: 1.0\n",
            "F1 Score: 0.6666666666666666\n",
            "Accuracy: 0.5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/pandas/core/arraylike.py:396: RuntimeWarning: invalid value encountered in sqrt\n",
            "  result = getattr(ufunc, method)(*inputs, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/pandas/core/arraylike.py:396: RuntimeWarning: invalid value encountered in sqrt\n",
            "  result = getattr(ufunc, method)(*inputs, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/pandas/core/arraylike.py:396: RuntimeWarning: invalid value encountered in sqrt\n",
            "  result = getattr(ufunc, method)(*inputs, **kwargs)\n"
          ]
        }
      ]
    }
  ]
}